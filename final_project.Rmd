---
title: "Final Project"
author: "Michael Dunst & Sofia Fasullo"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
knit: (function(input, encoding) {
    rmarkdown::render(
      input = input,
      encoding = encoding,
      envir = globalenv()
    )
  })    
---
**Project Description**: Airbnb has become a giant in the hospitality industry by offering ordinary property owners a chance to cash in on their home by renting it out by the night. We came forward to create a more helpful model than Airbnb currently provides to give prospective hosts an accurate forecast of what their home could be worth to a jetsetting visitor.

By creating this model, then packaging it in an easy to use app, homeowners can easily input their home's attributes and get an estimate. We, on the other hand, will quickly become a vital part of Airbnb's business and be a ripe target for acquisition and buyout.

```{r setup, echo=FALSE, install= TRUE, cache=TRUE, message=FALSE, results=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
# Load some libraries
rm(list = ls())
library(tidycensus)
library(dplyr)
library(viridis)
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(stargazer)
library(readr)
library(lubridate)
options(scipen=999) #scientific notation off

# Functions and data directory
census_api_key("8c8e36c4b5046c4d7f8a5d9f0f7a7d0ddde86e8b")

root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#981FAC","#CB0F8B","#FF006A","#FE4C35","#FE9900")
palette4 <- c("#981FAC","#FF006A","#FE4C35","#FE9900")
palette2 <- c("#981FAC","#FF006A")

```

###Data Collection and Explortation

```{r import listings}

listings <- read_csv("https://github.com/SofiaFasullo/MUSA508-final-project/raw/main/listings.csv")

listing_details <- read_csv("https://github.com/SofiaFasullo/MUSA508-final-project/raw/main/listings_details.csv")
```

```{r clean_data cache=TRUE}

listing_details <- listing_details %>%
  select(1,23,26,29,39,40,49,50,52:57,60,61,80,96)

listings <- listings %>%
  select(1)

full_listings <- merge(listings,listing_details,by="id")

full_listings <- full_listings %>% mutate(price = price %>% str_remove_all("[$,]") %>% as.numeric())

full_listings.sf <- st_as_sf(full_listings,coords=c("longitude","latitude"),crs=4326)
```

At first, we looked at a few sets of attributes to get a general sense of how different variables associate with each other.

One of the stronger correlations is between user experience and nightly rate. One might notice that a prospective host wouldn't have a user experience score to incorporate into a prediction, but we'll come back to that.

The next chart compares bedrooms and price. Unlike in a typical housing market, these is no continuous upward trend as the number of bedrooms increases. Perhaps this is due to the fact that center city apartments tend to be smaller yet more desirable.

Next, looking at responsiveness, there again isn't a clear trend, except maybe a correlation between higher prices and longer (!) respones times.

Lastly, we looked at the makeup of real estate and found an overwhelming portion of Airbnbs are "apartments". Looking at the average rate by home type, though, there were no interesting findings except for the price premium to stay in a lighthouse!

```{r data_exploration, cache = TRUE, message = FALSE, warning = FALSE}
ggplot(full_listings, aes(review_scores_rating,price))+
  geom_jitter(height=1,width=1)+
  xlim(50,100)+
  ylim(0,1000)+
  labs(x="User Ratings", y="Nightly Rate",
             title = "User Experience vs. Price",
             subtitle = "Amsterdam Airbnbs")+ 
  theme_minimal()
#General increase in price as ratings go up.
  
ggplot(full_listings, aes(bedrooms,price))+ 
  geom_bar(position = "dodge", stat = "summary", fun = "mean", fill="darkgreen")+
  labs(x="Bedrooms", y="Nightly Rate",
             title = "Number of Bedrooms vs. Price",
             subtitle = "Amsterdam Airbnbs")+ 
  theme_minimal()
#No general trend by number of bedrooms.

full_listings %>%
  drop_na(host_response_time) %>% #This drop na isn't working as intended
ggplot(., aes(host_response_time,price))+ 
  geom_bar(position = "dodge", stat = "summary", fun = "mean", fill="darkgreen")+
  labs(x="Host response time", y="Nightly Rate",
             title = "Host Eagerness to Rent vs. Price",
             subtitle = "Amsterdam Airbnbs")+ 
  theme_minimal()
#No association between responsiveness and nightly rate.

full_listings %>% 
	group_by(property_type) %>% 
	summarise(count = n()) %>% 
	ggplot(aes(x = reorder(property_type,(count)), y = count)) + 
		geom_bar(stat = 'identity', fill="darkgreen") + 
		theme_minimal()+
  scale_y_log10()+
  labs(x="Property Type", y="Count (Log-scaled)",
             title = "Types of Airbnb Properties",
             subtitle = "Amsterdam")+
  coord_flip()

ggplot(full_listings, aes(property_type,price))+ 
  geom_bar(position = "dodge", stat = "summary", fun = "mean", fill="darkgreen")+
  labs(x="Property Type", y="Nightly Rate",
             title = "Type of Property vs. Price",
             subtitle = "Amsterdam Airbnbs")+
  coord_flip()+
  theme_minimal()
#Nothing sticks out...except lighthouses!
```

To get a general sense of the city, this is a map of the neighborhoods that each property is categorized into.

```{r new_features, cache = TRUE, message = FALSE, warning = FALSE}
AMS_neighborhoods.sf <- 
  st_read("https://github.com/SofiaFasullo/MUSA508-final-project/raw/main/neighbourhoods.geojson")

ggplot() +
  geom_sf(data = AMS_neighborhoods.sf, aes(fill=neighbourhood))+
  labs(title = "Amsterdam Neighborhoods")+
  theme_minimal()
```

```{r monuments, cache = TRUE, message = FALSE, warning = FALSE}
monuments.sf <- st_read("https://github.com/SofiaFasullo/MUSA508-final-project/raw/main/landmarks.json")

#Change CRS to meters
full_listings.sf <- st_transform(full_listings.sf, crs = "EPSG:28992")
AMS_neighborhoods.sf <- st_transform(AMS_neighborhoods.sf, crs = "EPSG:28992")
monuments.sf <- st_transform(monuments.sf, crs = "EPSG:28992")

full_listings.sf <-
  full_listings.sf %>% 
    mutate(
      monuments_nn3 = nn_function(st_coordinates(full_listings.sf), 
                              st_coordinates(monuments.sf), k = 3))
```

Airbnb tends to be popular with tourists who are likely to pay more to stay close to what they want to visit. So, we brought in the locations of the top 20 landmarks in Amsterdam in order to rate each property's ease of access to where people want to go.

As you can see, there is a strong negative correlation between average distance to the nearest three landmarks and nightly rate.

```{r graph_monuments_data, cache = TRUE, message = FALSE, warning = FALSE}
ggplot()+
  geom_sf(data = AMS_neighborhoods.sf, fill="gray90")+
  geom_sf(data=monuments.sf, color="darkgreen")+
  ggtitle("Top 20 Landmarks", subtitle = "Amsterdam, Netherlands") +
  theme_minimal()

ggplot(full_listings.sf, aes(monuments_nn3,price))+
  geom_jitter(height=0.5,width=0.5)+
  xlim(0,7500)+
  ylim(0,1000)+
  labs(x="Avg. Distance to 3 Landmarks (m)", y="Nightly Rate",
             title = "Location vs. Price",
             subtitle = "Amsterdam Airbnbs")+ 

  geom_point(data=full_listings.sf,mapping=aes(monuments_nn3,price))+
  geom_smooth(method="lm")+
  stat_cor(method="pearson",
           label.x=4500,
           label.y=950) +
  labs(x="Distance From Monuments", y="Nightly Rate",
             title = "Monuments in Vicinity vs. Price",
             subtitle = "Amsterdam Airbnbs")+
  theme_minimal()
```

## Data Engineering

Regression models assume that the data being put into them is normally distributed. To make our data more useful and applicable to this assumption, we can transform it by taking the "log" of price to find a normal distribution.

``` {r transform_variable, cache = TRUE, message = FALSE, warning = FALSE}
ggplot()+
  geom_histogram(data=full_listings.sf,mapping=aes(price)) +
  labs(x="Nightly Rate", y="Frequency",
             title = "Distribution of Nightly Rate Prices",
             subtitle = "Amsterdam Airbnbs")+
  theme_minimal()

ggplot()+
  geom_histogram(data=full_listings.sf,mapping=aes(log10(price))) +
  labs(x="Nightly Rate", y="Frequency",
             title = "Distribution of Logarithmic Transformation",
             subtitle = "Amsterdam Airbnbs")+
  theme_minimal()

#clearly, taking the log of price normalizes the variable, wwhich is an assumption of regression models

#create a new variable of the log of price
full_listings.sf = full_listings.sf %>% mutate(
  logPrice = log10(price)
)
```

Another piece of feature engineering is turning the host's join date into simply a year. This means the data goes from a string to an integer, something that is more useful for our model. Also, looking at the distribution of host longevity and rate, we can see that newer hosts achieve lower rates than longer-tenured ones. Using this correlation, we can use an assumption for prospective hosts that they will have a lesser rating.

```{r extract_year_from_date, cache = TRUE, message = FALSE, warning = FALSE }
length(unique(full_listings.sf$host_since))
#this variable is different for every observation so it is not meaningful, simplifying it to year would be more helpful

full_listings.sf = full_listings.sf %>% mutate(
  year = lubridate::year(host_since)
)

#full_listings.sf = full_listings.sf %>% mutate(
#  superhost = ifelse(host_is_superhost=="t",1,0),
#  year = as.numeric(format(as.Date(full_listings.sf$host_since, format="%Y/%m/%d"),"%Y")))

ggplot(full_listings.sf, aes(year,review_scores_rating))+
  geom_jitter(height=1,width=1)+
#  xlim(50,100)+
#  ylim(0,1000)+
  labs(x="Year Joined", y="User Ratings",
             title = "Host Experience vs. Visitor Rating",
             subtitle = "Amsterdam Airbnbs")+ 
  theme_minimal()

```

#Fitting a "Kitchen Sink" Model
```{r partion_data, cache = TRUE, message = FALSE, warning = FALSE }
#split dataset into training and testing before fitting regression model

set.seed(111)
train = createDataPartition(y=full_listings.sf$logPrice, times = 1, p = 0.5, list=FALSE)
listings_train = full_listings.sf[train,]
listings_test = full_listings.sf[-train,]
```

```{r kitchen_sink_model, cache = TRUE, message = FALSE, warning = FALSE }

#because there are so many variables it would take up lots of space to display the whole summary, so we will just display the important parts in a table comparing models

#host_since and monuments_nn3 are not useful, neighborhood has too many NAs, and price is directly correlated to logPrice

fullOLS.lm = lm(logPrice~.,data= dplyr :: select(listings_train,-c(host_since, neighbourhood, monuments_nn3, price)) %>% st_drop_geometry() %>%  na.omit())

summary_fullOLS = summary(fullOLS.lm)
stargazer(data=fullOLS.lm, type="text", title = "Summary Statistics for Kitchen-Sink Model")

```


#Assessing Which Variables to Use
```{r corr_table, cache = TRUE, message = FALSE, warning = FALSE}
numericVars <- select_if(st_drop_geometry(full_listings.sf), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1),
  p.mat = cor_pmat(numericVars),
  colors = c("deepskyblue", "grey100", "firebrick1"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation Matrix of Numeric Variables", tl.cex = 0.5, tl.col = "black") +
  plotTheme()

```

#Fitting Models based on Selective Variables
```{r selective_model, cache = TRUE, message = FALSE, warning = FALSE }
#the model based on our diagnostics that would be useful
#the way we calculated n made it perfectly collinear with the neighborhood, wwhich is not helpful for us


trimmedOLS.lm = lm(logPrice~host_response_time+host_is_superhost+neighbourhood_cleansed+bedrooms+review_scores_rating+reviews_per_month+monuments_nn3+year, data=st_drop_geometry(listings_train) %>% drop_na())

summary_trimmedOLS = summary(trimmedOLS.lm)

stargazer(data=trimmedOLS.lm, type="text", title = "Summary Statistics for Model Based on Selected Variables", coef.omit="none")
#result: trying to use only the variables we think are good makes the R^2 drop a lot
```


#Spatial Clustering of Price - the "Secret Sauce" to a Model
```{r nearest_neighbor, cache = TRUE, message = FALSE, warning = FALSE }
#basic understanding of price distribution

ggplot() +
  geom_sf(data = full_listings.sf, aes(color=q5(logPrice)), legend=FALSE)
#well this isn't helpful!

#aggregate price by neighborhood instead
AMS_neighborhoods.sf = full_listings.sf %>% 
  group_by(neighbourhood_cleansed) %>% 
  summarize(avg_price_nhood = mean(price)) %>% 
  st_drop_geometry() %>% 
  right_join(.,AMS_neighborhoods.sf, by=c("neighbourhood_cleansed"="neighbourhood"))

palette5.2 <- c("#d7191c",
"#fdae61",
"#ffffbf",
"#a6d96a",
"#1a9641")

ggplot() +
  geom_sf(data = AMS_neighborhoods.sf , aes(fill=q5(avg_price_nhood), geometry = geometry))+
  scale_fill_manual(values = rev(palette5.2),
                   labels=qBr(AMS_neighborhoods.sf,"avg_price_nhood"),
                   name="Quintile\nBreaks")+
  labs(title="Nightly Rate by Neighborhood", subtitle="Amsterdam Airbnbs") +
  theme_minimal()
  #not sure why this isn't working
  
  
```


```{r map_lagPrice, cache = TRUE, message = FALSE, warning = FALSE }
coords <- st_coordinates(full_listings.sf) 

neighborList <- knn2nb(knearneigh(coords, 5)) #5 nearest neighbors

spatialWeights <- nb2listw(neighborList, style="W")

full_listings.sf$lagPrice <- lag.listw(spatialWeights, full_listings.sf$price) 

ggplot(data=full_listings.sf,mapping = aes(x=lagPrice,y=price)) + 
  geom_point() + geom_smooth(method="lm") + stat_cor(method="pearson") +
  xlim(0,1500) + ylim(0,1500)+
  labs(x="Spatial Lag of Price", y="Price",
             title = "Spatial Correlation of Nightly Rate Price",
             subtitle = "Amsterdam Airbnbs")+
  theme_minimal()

```

```{r model_spatial_lag, cache = TRUE, message = FALSE, warning = FALSE }
set.seed(111)
listings_train = full_listings.sf[train,]
listings_test = full_listings.sf[-train,]

smarter_model = lm(logPrice ~ log10(lagPrice)+host_response_time+host_is_superhost+neighbourhood_cleansed+bedrooms+review_scores_rating+reviews_per_month+monuments_nn3+year, data=st_drop_geometry(drop_na(listings_train)))

stargazer(data=smarter_model, type="text", title = "Summary Statistics for Model Using Spatial Lag of Price")
```

```{r kitchen_sink_spatial_model, cache = TRUE, message = FALSE, warning = FALSE }
#kitchen sink with log of price
smarter_model2 = lm(logPrice ~., data=st_drop_geometry(drop_na(select(listings_train,-price))))
summary(smarter_model)

```

#Diagnostics for Generalizability and Accuracy
```{r diagnostics, cache = TRUE, message = FALSE, warning = FALSE }
#smarter_model seems like our best bet
#accuracy test: error for test dataset
MSE = mean((exp(listings_test$logPrice) - exp(predict(smarter_model,data=st_drop_geometry(drop_na(select(listings_train,-price))))))^2)
#MSE #6 mean square error is pretty good?!
MAE = mean(abs(exp(listings_test$logPrice) - exp(predict(smarter_model,data=st_drop_geometry(drop_na(select(listings_train,-price)))))))
MAE

#vs our model without the secret sauce
MSE_1stmod = mean((exp(listings_test$logPrice) - exp(predict(fullOLS.lm,data=st_drop_geometry(drop_na(select(listings_train,-c(price,lagPrice))))))^2))
#MSE_1stmod
MAE_1stmod = mean(abs(exp(listings_test$logPrice) - exp(predict(fullOLS.lm,data=st_drop_geometry(drop_na(select(listings_train,-c(price,lagPrice))))))))
MAE_1stmod
#first model with no data engineering and variable selection is better with accuracy BUT


#generalizability test 

fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(price~ ., data = st_drop_geometry(full_listings.sf) %>%                       dplyr::select(lagPrice,host_response_time,host_is_superhost,neighbourhood_cleansed,bedrooms,review_scores_rating,reviews_per_month,price) %>% na.omit(), 
     method = "lm", trControl = fitControl, na.action = na.omit)

reg.cv


first.cv <- 
  train(price ~ ., data = st_drop_geometry(full_listings.sf) %>%                                dplyr::select(-c(logPrice,lagPrice)) %>% drop_na(), 
     method = "lm", trControl = fitControl, na.action = na.pass)
first.cv

#so cross validation R^2 accuracy is better in the first model with more predictors but the error of the cross validated tests is lower for our tuned model, so we are favoring slightly less accuracy to higher generalizability
```























