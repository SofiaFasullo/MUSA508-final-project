---
title: "Final Project"
author: "Michael Dunst & Sofia Fasullo"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
knit: (function(input, encoding) {
    rmarkdown::render(
      input = input,
      encoding = encoding,
      envir = globalenv()
    )
  })    
---

```{r setup, echo=FALSE, install= TRUE, cache=TRUE, message=FALSE, results=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
# Load some libraries
rm(list = ls())
library(tidycensus)
library(dplyr)
library(viridis)
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(stargazer)
library(readr)
options(scipen=999) #scientific notation off

# Functions and data directory
census_api_key("8c8e36c4b5046c4d7f8a5d9f0f7a7d0ddde86e8b")

root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#981FAC","#CB0F8B","#FF006A","#FE4C35","#FE9900")
palette4 <- c("#981FAC","#FF006A","#FE4C35","#FE9900")
palette2 <- c("#981FAC","#FF006A")

```



```{r import listings}

listings <- read_csv("https://github.com/SofiaFasullo/MUSA508-final-project/raw/main/listings.csv")

listing_details <- read_csv("https://github.com/SofiaFasullo/MUSA508-final-project/raw/main/listings_details.csv")
```

```{r clean data}

listing_details <- listing_details %>%
  select(1,23,26,29,39,40,49,50,52:57,60,61,80,96)

listings <- listings %>%
  select(1)

full_listings <- merge(listings,listing_details,by="id")

full_listings <- full_listings %>% mutate(price = price %>% str_remove_all("[$,]") %>% as.numeric())

full_listings.sf <- st_as_sf(full_listings,coords=c("longitude","latitude"),crs=4326)
```



```{r data exploration}
ggplot(full_listings, aes(review_scores_rating,price))+
  geom_jitter(height=1,width=1)+
  xlim(50,100)+
  ylim(0,1000)+
  labs(x="User Ratings", y="Nightly Rate",
             title = "User Experience vs. Price",
             subtitle = "Amsterdam Airbnbs")+ 
  theme_minimal()
#General increase in price as ratings go up.
  
ggplot(full_listings, aes(bedrooms,price))+ 
  geom_bar(position = "dodge", stat = "summary", fun = "mean", fill="darkgreen")+
  labs(x="Bedrooms", y="Nightly Rate",
             title = "Number of Bedrooms vs. Price",
             subtitle = "Amsterdam Airbnbs")+ 
  theme_minimal()
#No general trend by number of bedrooms.

full_listings %>%
  drop_na(host_response_time) %>% #This drop na isn't working as intended
ggplot(., aes(host_response_time,price))+ 
  geom_bar(position = "dodge", stat = "summary", fun = "mean", fill="darkgreen")+
  labs(x="Host response time", y="Nightly Rate",
             title = "Host Eagerness to Rent vs. Price",
             subtitle = "Amsterdam Airbnbs")+ 
  theme_minimal()
#No association between responsiveness and nightly rate.

full_listings %>% 
	group_by(property_type) %>% 
	summarise(count = n()) %>% 
	ggplot(aes(x = reorder(property_type,(count)), y = count)) + 
		geom_bar(stat = 'identity', fill="darkgreen") + 
		theme_minimal()+
  scale_y_log10()+
  labs(x="Property Type", y="Count (Log-scaled)",
             title = "Types of Airbnb Properties",
             subtitle = "Amsterdam")+
  coord_flip()

ggplot(full_listings, aes(property_type,price))+ 
  geom_bar(position = "dodge", stat = "summary", fun = "mean", fill="darkgreen")+
  labs(x="Property Type", y="Nightly Rate",
             title = "Type of Property vs. Price",
             subtitle = "Amsterdam Airbnbs")+
  coord_flip()+
  theme_minimal()
#Nothing sticks out...except lighthouses!
```
some notes:
lighthouse looks like an outlier

```{r new features}
AMS_neighborhoods.sf <- 
  st_read("https://github.com/SofiaFasullo/MUSA508-final-project/raw/main/neighbourhoods.geojson")

ggplot() +
  geom_sf(data = AMS_neighborhoods.sf, aes(fill=neighbourhood))
```

```{r monuments}

monuments.sf <- st_read("https://github.com/SofiaFasullo/MUSA508-final-project/raw/main/landmarks.json")

#Change CRS to meters
full_listings.sf <- st_transform(full_listings.sf, crs = "EPSG:28992")
AMS_neighborhoods.sf <- st_transform(AMS_neighborhoods.sf, crs = "EPSG:28992")
monuments.sf <- st_transform(monuments.sf, crs = "EPSG:28992")

#neither of these worked!

full_listings.sf <-
  full_listings.sf %>% 
    mutate(
      monuments_nn3 = nn_function(st_coordinates(full_listings.sf), 
                              st_coordinates(monuments.sf), k = 3))

  



```


```{r graph monument}

ggplot()+
  geom_point(data=full_listings.sf,mapping=aes(monuments_nn3,price))+
  labs(x="Distance From Monuments", y="Nightly Rate",
             title = "Monuments in Vicinity vs. Price",
             subtitle = "Amsterdam Airbnbs")+
  theme_minimal()

ggplot()+
  geom_histogram(data=full_listings.sf,mapping=aes(price)) +
  labs(x="Nightly Rate", y="Frequency",
             title = "Distribution of Nightly Rate Prices",
             subtitle = "Amsterdam Airbnbs")+
  theme_minimal()

ggplot()+
  geom_histogram(data=full_listings.sf,mapping=aes(log10(price))) +
  labs(x="Nightly Rate", y="Frequency",
             title = "Distribution of Logarithmic Transformation",
             subtitle = "Amsterdam Airbnbs")+
  theme_minimal()

#clearly, taking the log of price normalizes the variable, wwhich is an assumption of regression models

#create a new variable of the log of price
full_listings.sf = full_listings.sf %>% mutate(
  logPrice = log10(price)
)
```



```{r corr table}
numericVars <- select_if(st_drop_geometry(full_listings.sf), is.numeric) %>% na.omit()

ggcorrplot(
  round(cor(numericVars), 1),
  p.mat = cor_pmat(numericVars),
  colors = c("deepskyblue", "grey100", "firebrick1"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation Matrix of Numeric Variables", tl.cex = 0.5, tl.col = "black") +
  plotTheme()



```
#some feature engineering
```{r}
#full_listings.sf = full_listings.sf %>% mutate(
# superhost = ifelse(host_is_superhost=="t",1,0),
#  year = as.numeric(separate(host_since,into=c('year'),sep=(4)))
#  )
# error ! no applicable method for 'separate' applied to an object of class "Date"
```


```{r}
#should split dataset into training and testing before

train = createDataPartition(y=full_listings.sf$logPrice, times = 1, p = 0.5, list=FALSE)
listings_train = full_listings.sf[train,]
listings_test = full_listings.sf[-train,]

fullOLS.lm = lm(logPrice~.,data=st_drop_geometry(select(listings_train,-price)))
#summary(fullOLS.lm)#this makes me cry

#variable thoughts
#host_since turn into time been a host
#host it superhost turn into 0-1
#delete neighborhood, lat, long, square feet

fullOLS.lm2 = lm(logPrice~.,data= listings_train %>% dplyr :: select(-c(host_since, neighbourhood, monuments_nn3, price)) %>% st_drop_geometry() %>%  na.omit())
summary(fullOLS.lm2)
```

```{r}
#the model based on our diagnostics that would be useful
#the way we calculated n made it perfectly collinear with the neighborhood, wwhich is not helpful for us

trimmedOLS.lm = lm(logPrice~host_response_time+host_is_superhost+neighbourhood_cleansed+bedrooms+review_scores_rating+reviews_per_month, data=st_drop_geometry(listings_train) %>% drop_na())
summary(trimmedOLS.lm)
#result: trying to use only the variables we think are good makes the R^2 drop a lot
```

```{r}
#some notes to move forward!
#Chapter 4 of the textbook talks about fixed neighborhood effects and how to account for them (oour variable n is a fixed neighborhood effect)
#chapter 4 also goes into predicting price using spatial clustering of price
```

#spatial clustering of price
```{r}
#basic understanding of price distribution

ggplot() +
  geom_sf(data = full_listings.sf, aes(color=q5(logPrice)), legend=FALSE)
#well this isn't helpful!

#aggregate price by neighborhood instead
AMS_neighborhoods.sf = full_listings.sf %>% 
  group_by(neighbourhood_cleansed) %>% 
  summarize(avg_price_nhood = mean(price)) %>% 
  st_drop_geometry() %>% 
  right_join(.,AMS_neighborhoods.sf, by=c("neighbourhood_cleansed"="neighbourhood"))

ggplot() +
  geom_sf(data = AMS_neighborhoods.sf , aes(fill=avg_price_nhood)) #not sure why this isn't working
  
  
```


```{r}
coords <- st_coordinates(full_listings.sf) 

neighborList <- knn2nb(knearneigh(coords, 5)) #5 nearest neighbors

spatialWeights <- nb2listw(neighborList, style="W")

full_listings.sf$lagPrice <- lag.listw(spatialWeights, full_listings.sf$price) 

ggplot(data=full_listings.sf,mapping = aes(x=lagPrice,y=price)) + 
  geom_point() + geom_smooth(method="lm") + stat_cor(method="pearson") +
  xlim(0,1500) + ylim(0,1500)+
  labs(x="Spatial Lag of Price", y="Price",
             title = "Spatial Correlation of Nightly Rate Price",
             subtitle = "Amsterdam Airbnbs")+
  theme_minimal()

```

```{r}
#recall train and testing
listings_train = full_listings.sf[train,]
listings_test = full_listings.sf[-train,]

smarter_model = lm(logPrice ~ log10(lagPrice)+host_response_time+host_is_superhost+neighbourhood_cleansed+bedrooms+review_scores_rating+reviews_per_month, data=st_drop_geometry(drop_na(listings_train)))
summary(smarter_model)
```

```{r}
smarter_model2 = lm(logPrice ~., data=st_drop_geometry(drop_na(select(listings_train,-price))))
summary(smarter_model)
```

#Diagnostics for Generalizability and Accuracy
```{r}
#smarter_model seems like our best bet
#accuracy test: error for test dataset
MSE = mean((exp(listings_test$logPrice) - exp(predict(smarter_model,data=st_drop_geometry(drop_na(select(listings_train,-price))))))^2)
#MSE #6 mean square error is pretty good?!
MAE = mean(abs(exp(listings_test$logPrice) - exp(predict(smarter_model,data=st_drop_geometry(drop_na(select(listings_train,-price)))))))
MAE

#vs our model without the secret sauce
MSE_1stmod = mean((exp(listings_test$logPrice) - exp(predict(fullOLS.lm,data=st_drop_geometry(drop_na(select(listings_train,-c(price,lagPrice))))))^2))
#MSE_1stmod
MAE_1stmod = mean(abs(exp(listings_test$logPrice) - exp(predict(fullOLS.lm,data=st_drop_geometry(drop_na(select(listings_train,-c(price,lagPrice))))))))
MAE_1stmod
#first model with no data engineering and variable selection is better with accuracy BUT


#generalizability test 
kfolds <- trainControl(method = "cv", number = 20)
set.seed(825)

reg.cv <- 
  train(logPrice ~ ., data = st_drop_geometry(full_listings.sf) %>%                                dplyr::select(lagPrice,host_response_time,host_is_superhost,neighbourhood_cleansed,bedrooms,review_scores_rating,reviews_per_month,logPrice) %>% drop_na(), 
     method = "lm", trControl = kfolds, na.action = na.pass)

reg.cv

first.cv <- 
  train(logPrice ~ ., data = st_drop_geometry(full_listings.sf) %>%                                dplyr::select(-c(price,lagPrice)) %>% drop_na(), 
     method = "lm", trControl = kfolds, na.action = na.pass)
first.cv
```























